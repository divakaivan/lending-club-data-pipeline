# Visualisation dashboard

#### Click [here](https://lookerstudio.google.com/reporting/de05bd99-c678-4088-8abd-fa0974c0da0e) for an interactive version in Looker

![Visualisation dashboard](/project_info/dashboard.png)

# Project diagram

![Project diagram](/project_info/project_diagram.png)

- Raw **Lending Club** data comes from [Kaggle](https://www.kaggle.com/datasets/gabrielsantello/lending-club-loan-preprocessed-dataset)
- **Mage** is used to orchestrate an end to end process including:
  - extract data using kaggle's API and load it to the **Google Cloud Storage** (used as a data lake)
  - create tables in **BigQuery** (used as a data warehouse)
  - run dbt transformation jobs
- **Terraform** is used to manage and provision the infrastructure needed for your data pipeline on Google Cloud Platform
- **dbt** is used to transform the data into dimension tables, add data tests, and create [documentation](https://lending-club-project-dbt-docs.netlify.app/)
- **Looker** is used to create a visualisation [dashboard](https://lookerstudio.google.com/reporting/de05bd99-c678-4088-8abd-fa0974c0da0e)

# Data lineage overview

#### For a full view, visit the [project's data documentation](https://lending-club-project-dbt-docs.netlify.app/) generated by dbt.

![Data lineage overview](/project_info/data_model.png)

# Mage pipeline overview  

The below pipeline takes raw data from kaggle and outputs data ready to be visualised in Looker. 

![Mage pipeline overview](/project_info/mage_end_to_end_overview.png)

# Reproducability

1. Clone the repository
```
https://github.com/divakaivan/lending-club-data-pipeline.git
```

2. Go to the repository folder in your terminal, and type `make`

3. Follow the on-screen instructions to set up GCP resources and start Mage (`http://localhost:6789/`)
```
If running for the first time, run in order 1~5
Usage: make [option]

Options:
  help                 Show this help message
  gcp-tf-init          1. Initialize GCP resources
  gcp-tf-plan          2. See GCP resources to be created
  gcp-tf-apply         3. Create GCP resources
  docker-build         4. Build Mage environment
  docker-up            5. Start Mage environment
  docker-down          6. Stop Mage environment

Make sure to place your kaggle.json and gcp-creds.json files in terraform/keys/ so that Terraform and Mage can access them.
```

### Things to consider for improvements

- increase data volume: at the moment it is using a dataset with ~400K observations.
- data modelling: at the moment the final result are only 3 tables related to loans, borrower and date; more complex data models can be created
- automate documentation hosting: the current one was hosted manually with Netlify file upload
- creating a more complex Looker dashboard
